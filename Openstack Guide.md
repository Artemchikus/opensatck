**Мой дневник по практике**

\1. Подготовительная работа для создания ВМ.

1.1 Проверка поддержки виртуализации, если выртуализация есть, то должен быть такой вывод.

1.2. Устнаовка необходимых пакетов: [**bridge-utils**](https://linuxhint.com/bridge-utils-ubuntu/) (создание моста), [**libvirt-daemon-system**](https://packages.debian.org/unstable/libvirt-daemon-system) (создание ВМ), [**quemu и quemu-kvm**](https://redos.red-soft.ru/base/arm/os-virtual/qemu-kvm/qemu-kvm/) (работа гипервизора).

1.3. Активация libvirtd.service для дальнейшей работы.

1.4. Во всяких [**гайдах**](https://ubuntu-admin.ru/ubuntu-kvm/) говорится, что есть три способа настройки доступа виртуальных машин в сеть: чрез одну из ВМ (самый труднонастраеваемый и еще доп ВМ нужна), через NAT (уже реализован по дефлоту virbr0), через Bridge (самый обычный и удобный, его и реализуем, не зря ведь bridge-utils качали).

Для этого создаем сетевой интерфейс br0 типа bridge.

Потом все удаляем так как [**оказывается**](https://unix.stackexchange.com/questions/554331/theoretical-tap-interface-w-wifi-parent-interface), что нельзя проводить мост к wifi сети, только к etherner, потому что Wi-Fi требует дополнительных MAC-адресов для связи, из-за чего принято решение пользоваться NAT :-)

1.5. Установка софта для обновления времени сервера [**chrony**](https://chrony.tuxfamily.org/), для синхронизации времени вритуалок.


\2.  Установка обзаза Centos и создание 3 ВМ с помощью KVM.

2.1. Создание диска с пмощью [**qemu-img**](https://docs.altlinux.org/ru-RU/alt-server-v/9.2/html/alt-server-v/ch46s03.html) формата [**qcow2**](https://ru.wikipedia.org/wiki/Qcow2) для монтирования в ВМ (**preallocation=metadata** – выделяет пространство, необходимое для метаданных, но не выделяет место для данных. Это самый быстрый способ предоставления, но самый медленный для гостевой записи). Папки создал сам, кроме mnt.

2.2. Установка утилит [**virt-install](https://docs.altlinux.org/ru-RU/alt-server-v/9.2/html/alt-server-v/ch46s02.html)** (создание ВМ) и [**osinfo-query](https://man.archlinux.org/man/community/libosinfo/osinfo-query.1.en)** (получение информации о поддержке гипервизорами различных операционных систем) для для создания гостевых машин в среде kvm из командной строки.

2.3. Просмотр доступных для KVM образов Centos с помощью команды osinfo-query os.

2.4. Установка дистрибутива операционной системы с сайта <http://mirror.yandex.ru/centos/7.9.2009/isos/x86_64/> В моем случае загружается образ CentOS-7-x86\_64-Minimal-2009.iso. Оказалось, что Centos крутая штука, так как ворует все идеи у комерческой Red Hat Enterprise Linux, но при этом абсолютно бесплатная.

2.5.	 Cоздание ВМ с Centos7.0 с помщью virt-intsall ([**сайт**](https://libvirt.org/formatdomain.html#elementsOS) с документацией параметров).

- **--virt-type kvm** — тип гипервизора
- **--name VM1** — имя вритуалки
- **--ram 4096** — опреативка
- -**-arch=x86\_64** - имитируемая архитектура процессора
- -**-disk /mnt/kvm/disk/vm1.qcow2,size=6,format=qcow2** — простарнство хранения данных, **size** — размер, **format** — формат тома.
- **--network network=default** — сетевой интерфейс (**default=virbr0**)
- -**-os-type=linux** — тип операциооной системы
- **--os-variant=centos7.0** — конкретный вариант ОС
- **--location=/mnt/kvm/iso/CentOS-7-x86\_64-Minimal-2009.iso** — путь до установочного образа.
- **--graphics none** - настройки экрана ВМ (**none** — без графического интерфейса)
- **--console pty,target\_type=serial** — тип подкюченя к гостевой консоли (**pty** - обеспечивают работу терминала в оконном интерфейсе, **target\_type=serial** — дефолтная консоль, не открывается новое окно)
- **--extra-args 'console=ttyS0,115200n8 serial'** — дополнительные аргументы, в моем случае парметры консоли в которую будет выводится информация иначе не видать меню загрузки.

2.6. Настройка с помощью меню устновки Centos без графического интерфейса (в первый раз не понял,что за фигня и как этим пользоваться, оказывается циферками). Все загрузится ноомально если на месте восклицательных знаков будут крестики.



Настройка сети такая (почему-то скрин поплыл, но там DNS и Gatway одинаковые — 192.168.122.1):

Заполненное меню загрузки такое, пользователя не создаем:

Готовая ВМ:

2.7. Полезные команды для работы с ВМ:

- **virsh list --all** — просмотр всех ВМ
- **virsh start vm** — запустить неактивную ВМ
- **virsh shutdown vm** — выключить ВМ через гостевую ОС
- **virsh destroy vm** — выключить ВМ принудительно (выдернуть из разетки)
- **virsh domblklist vm** — информация о дисках ВМ
- **virsh dominfo vm** — информация о ВМ
- **virsh nodeinfo** — инфоомация о хостовой машине
- **virsh undefine --domain vm** — удалить всю информацию о ВМ, например чтобы переиспользовать диск который ей был занят (я то 3 раза ее пересоздавал)
- **virsh console vm** — подключение к консоли ВМ
- **virsh dumpxml vm** — информация о настройках вм
- **virsh snapshot-list --domain vm** — просмотр снапшотов ВМ
- **virsh snapshot-create-as vm vm-snapshot** — создание сапшота ВМ
- **virsh snapshot-info --domain vm --snapshotname vm-snapshot** — информация о спашоте ВМ
- **virsh snapshot-create-as --domain vm --name vm-snapshot** — создание снапшота ВМ
  - **--disk-only** — создане снапшота только диска (без состояния оперативки)
- **virsh snapshot-revert --domain vm --snapshotname vm-snapshot**  — откатить ВМ к снапшот
  - **--running —** запустить ВМ
- **virsh snapshot-delete --domain vm --snapshotname vm-snapshot** — удалить снапшот ВМ
- **virsh net-create path\_to\_net\_conf** — создать сеть для ВМ
- **virsh net-list** — список сетей
- **virsh net-autostart net** — настройка автостарта сети
- **virsh attach-interface vm --type bridge --source interface --model virtio --config --live --persistent** — присоеденение сетевого интерфеса к ВМ
  - **--source —** имя сетевого интерфеса на хосте
  - **--model —** тип интерфейса
  - **--config —** через файл конфига
  - **--live —** ВМ не выключать
  - **--persitent —** подключить навсегда
- **virsh detach-interface vm3 --mac '52:54:00:3b:6e:3f' --type bridge --persistent** — отсоедние сетевого интефейса от ВМ
- **virsh net-info net** — информация о сети
- **qemu-img info disk1.qcow2** — информация об образе диска
- **qemu-img resize /mnt/kvm/disk/vmserver01-disk1.qcow2 +1G** — увеличение размера диска
- **qemu-img info /mnt/kvm/disk/vmserver01-disk1.qcow2** — информация о диске
- **Ctrl + ]** - выход из гостевой консоли
- **virt-install --parameter=?** - информация о параметре (лучше сайтом пользоваться с команды пользы 0)
- **nmcli connection show** — показывает все сетевые интерфейсы кроме loopback
- **nmcli connection delete interface** — удаляет сетевой интерфейс (если вы выяснили, что мост в wifi не прокидывается)
- **sudo mv src dst** — перенести файл, если у вас почемуто нет прав на доступа к папке mnt через графический интерфейс Ubuntu

Отстальные 2 ВМ поднимаются по аналогичной схеме только с другими ip адресами, требованиями к оперативе и размеру диска.

\3. Установка и настройка RabbitMQ на Control узел.

3.1. Зачем он вообще нужен? Оказывается RabbitMQ или какой-нибудь другой брокер сообщений (Apache Qpid или ZeroMQ) используется для координирования операций и обмена информацией между сервисами OpenStack, такими как Glance, Cinder, Nova, OpenStack Networking, Heat и Ceilometer по протоколу AMQP (Advanced Message Queuing Protocol).

3.2. [**Установка RabbitMQ**](https://gist.github.com/fernandoaleman/fe34e83781f222dfd8533b36a52dddcc) на ВМ с помощью yum (-y — автоответ yes, если не знали).

Сноска от автора (дочитайте до конца!), пока я пытался установить RabiitMQ по гайдам, я умер внутри, но после долгих поисков мне все таки удалось найти [**нормальное руководство**](https://computingforgeeks.com/installing-rabbitmq-on-centos-6-centos-7/), при помощи которого RabbitMQ все таки удалось установить. Но я же проделал такю большую работу качая RabbitMQ по другим гайдам, так что было принято решение сохранить мои первые попытки в описание процесса установки, нормальный порядок установки находится во [**тут**](#вторая часть). **ВНИМАНИЕ!!!!!** Я публично извиняюсь перед Андреем Маркеловым (автор книги) за [**эти слова**](#sorry), и снимаю перед ним шляпу, так как его способ установки онсновых утилит для разворачиваняи опенстека является самым удобным в интренете, достаточно ввести 3 следующие команды после запуска ВМ и все остальные пакеты (сервисы Openstack и RabbitMQ) будут загружаться через обычный yum (таким образом настройка RabbitMQ начинается [**здесь**](#rabbit)):

3.3. Установка epel-release (открытое бесплатное хранилище пакетов от Fedora, видимо нужно для RabbitMQ) и обновление компонентов системы.

3.4. Установка Erlang - язык, на котром написан RabbitMQ. Для этого надо установить [**wget**](https://habr.com/ru/company/ruvds/blog/346640/), так как он не предустановлен, после чего загрузить rpm репозиторий с помощью wget, потом добавть его с помощью [**rpm**](https://www.inp.nsk.su/~bolkhov/teach/inpunix/setup_rpm.ru.html) -Uvh (U - апгрейд пакета, vh - для статус бара и дополнительной информации), и в конце установить необходимые пакеты ([**erlang**](https://habr.com/ru/post/50028/), [**socat**](https://linux-notes.org/ustanovka-socat-v-unix-linux/), [**logrotate**](https://1cloud.ru/help/linux/upravlenie-logami-s-pomoshch'yu-logrotate-na-ubuntu-16-04)) с помощью yum, на сколько я понял без них RabbitMQ не заработает, как минимум без eralng.

3.5. Устновка rabbitmq-server. Для этого устанавливаем rpm пакет с RabbitMQ с помощью wget, потом добавляем ключ подписи с помощью rpm (без него вылезет ошибка), после чего устанавливаем [**rabbitmq-server**](https://habr.com/ru/post/149694/) с помощью rpm, запускаем его и настариваем автозапуск.

Понимаем, что rabbitmq-server не запускается, а точнее находится в состоянии activating пока не словит timeout, и решаем эту проблему путем настройки firewall-cmd и [**SELinux**](https://habr.com/ru/company/kingservers/blog/209644/).



Для этого освобождаем следующие [**порты**](https://www.rabbitmq.com/networking.html) (гле-то говорится, что достаточно сделать публичным только базовый порт RabbitMQ, но на всякий и другие тоже опубликуем, хотя наверное можно просто вырубить фаерволл):

- **4369** — порт сервиса empd (Erlang Port Mapper Daemon), служба обнаружения одноранговых узлов, используемая узлами RabbitMQ и инструментами CLI.
- **25672** — gорт сервера распространения Erlang, используется для связи между узлами и инструментами CLI.
- **5671** — порт сервиса [**ampq**](https://habr.com/ru/post/62502/)  (Аdvanced Message Queuing Protocol), тот самый протокол обмена сообщениями, который использует RabbitMQ.
- **5672** — порт сервиса ampqs (amqp protocol over TLS/SSL), из расшифровки и так понятно (как говорится на офицальном сайте, RabbitMQ не работает если на машине нет OpenSSL 1.1, конфликты с Erlang 24 версии, но может быть все работает, я не проверял)
- **15672** — порт клиентов [**HTTP API**](https://www.rabbitmq.com/management.html), [**пользовательского интерфейса управления**](https://www.rabbitmq.com/management.html) и [**rabbitmqadmin**](https://www.rabbitmq.com/management-cli.html) без TLS/SSL, почему-то с TLS/SSL не публикуется.
- **61613** — порт сервиса [**stomp**](http://onreader.mdl.ru/RabbitMQInDepth/content/Ch09.html) (Simple Text Oriented Messaging Protocol), альтернатива AMPQ для спецефичных случаев.
- **61614** -  порт сервиса stomps (stomp protocol over TLS/SSL), аналогично ampq и ampqs.
- **1883** — порт сервиса [**mqtt**](http://onreader.mdl.ru/RabbitMQInDepth/content/Ch09.html) (Message Queue Telemetry Transport) альтернатива AMPQ для спецефичных случаев.
- **8883** — порт сервиса mqtts (mqtt protocol over TLS/SSL), аналогично ampq и ampqs.

- **--zone=public** — публичный доступ к порту
- **--permanent** — останется после перезагрузки машины

Перезагрузка фаерволла, порты добавились:

Включаем nis (Network Information Service - распространяет карты имён, паролей и другую важную информацию компьютерам своего домена) в SELinux. Совершенно без понятия зачем, видимо как-то связано с работой Erlang, если это не сделать, то порт для RabbitMQ не выделится.

После чего пробуем различные фиксы из интернета (около 6), обращаемся к гадалке, от отчаяния пробуем оригинальный гайд по установке с офицального сайта, и в конце концов понимаем, что ничего не работает. RabbitMQ убивает людей…

3.3. Итак вот оно — описание установки при помощи другого руководства. Первым делом меняем hostname, потому что RabbitMQ может не записутится, если hostname поменяется в будущем.

3.4. Далее как по [**старым гайдам**](#epel-releace) устанавливаем wget и epel-release.

3.5. Потом как по [**старым гайдам**](#erlang) устанавливаем Erlang, но на этот раз без socat и logrotate (видимо не так они и нужны), а также понимаем, что в прошлый раз скачали старую версию (erlang-solutions-1.0-1), из-за чего могли и возникнуть трудности.

3.6. На этом шаге уже стущественные отличия, а именно нужно создать и заполнить файл **/etc/yum.repos.d/rabbitmq.repo** (конфигурация репозитория), возможно в старых гайдах это производилось автоматически. Nano на Centos не предустоновлено так что пользуемся vi.

Обновляем кэш репозиториев yum и уже монжно наблюдать как все красиво:



3.7. Момент истины, загружаем rabbitmq-server и запускаем его. Загрузилось нормально ухх, включалось долго (чуть инфваркт не схватил), и….. (драматическая пауза) да Высшие Силы сжалились надо мной и все запустилось, ни фаервлолл ни SELinux настраивать (пока) не пришлось.

Если что вот так выглядело описание установки из книги (если вы сюда телепортировались, то сначала телепортируйтесь [**сюда**](#sorry)):

3.8. Постоянная рубрика «Удобные команды» на этот раз RabbitMQ (правда они вряд ли понадобятся, напрямую с RabbtMQ работает только Openstack, а нет понадобятся для настройки пользователя):

- **rabbitmqctl delete\_user user** — удалить пользователя
- **rabbitmqctl change\_password user strongpassword** — сменить пароль пользователя
- **rabbitmqctl add\_vhost /my\_vhost** — добавить [**virtualhost**](http://VirtualHost/)
- **rabbitmqctl list\_vhosts** — список virualhosts
- r**abbitmqctl delete\_vhost /my\_vhost** — удалить virtualhost
- **rabbitmqctl set\_permissions -p /my\_vhost user ".\*" ".\*" ".\*"** - дать разрешение пользователю внутри virtualhost
- **rabbitmqctl list\_permissions -p /my\_vhost** — список разрешений внутри virtualhost
- r**abbitmqctl list\_user\_permissions user** — список всех разрешений пользователя
- **rabbitmqctl clear\_permissions -p /my\_vhost user** — удалить разрешение пользователя внутри virtualhost
- **rabbitmqcrl status** — статус сервиса rabbitmq-server
- **rabbitmqctl create\_uers user stronpassword** — создать пользователя
- **rabbitmqctl list\_users** — список пользоватлей
- **rabbitmqctl set\_user\_tags user tag** — присвоит тэг пользователю

3.9. Оказвается я пропустил некоторые настройки из начала книги, а именно обновление всех установленных пакетов:

Отключение network manager и редактирование файла  ifcfg-eth0 (поправить параметры NM\_CONTROLLED и ONBOOT, а также проверить настройку ip адресса и тд) в папке **/etc/sysconfig/network-scripts/**, так как NetworkManager будет мешать работе Openstack Neutron, судя по [**документации RedHat**](https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/10/html/manual_installation_procedures/chap-prerequisites):



Отключение фаерволла, а я то мучался ([**firewalld не используется даже когда хочется**](https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/10/html/manual_installation_procedures/chap-prerequisites), вместо него используется iptables, будем их настраивать во время настройки Neutron):

Установка пакета openstack-selinux (**sudo** **yum -y install openstack-selinux**) или отключчение SELinux (я решил отключить (изменить поле SELINUX в файле **/etc/sysconfig/selinux**, потом перезагрузиться), так как не особо разбираюсь в данной технологии, но может быть изучу, года наконец разверну кластер):

Проверка после перезагрузки:

Установка chrony, как на хосте, у меня он уже был предустановлен и настроен (вроде бы устнавливал только на хост машину):

Установка [**crudini**](http://www.pixelbeat.org/programs/crudini/) для редактирования конфигурационных файлов:

Пример команды для редактирования конфига (не буду же я одну команду выносить в рубрику):

НО это все равно не меняет того, что способ установки RabbitMQ представленный в книге неполный, хотя и была показана установка репозитория epel-release (выполните две команды ниже и [**назад**](#rabbit)).

3.10. Ну теперь можно заняться настройкой RabbitMQ. Для начала настроим аунтефикацию, в руководствах говорится что есть два способа аунтефикации: с использованием имени и пароля SASL-аутентификации (Simple Authentication and Security Layer), обеспечиваемой фреймворком Erlang, и при помощи сертификатов и SSL. Я решил (посмотрел в руководствах), что буду реализовывать первый способ + все сервисы будут работать под одним пользователем RabbitMQ (да я знаю, что так не очень безопасно, но я и фаерволл уже вырубил). Что касается SASL-аутентификации в RabbitMQ, то есть два метода: [**PLAIN**](https://www.rabbitmq.com/access-control.html#:~:text=Description-,PLAIN,-SASL%20PLAIN%20authentication) и [**AMQPLAIN](https://www.rabbitmq.com/access-control.html#:~:text=most%20other%20clients.-,AMQPLAIN,-Non%2Dstandard%20version)** и дефолтный пользователь guest.

Создаем пользователя openstack/openstack для настройки сервисов Openstack и добавляем ему парава на настройку, чтение и запись:

Для дальнейшего удобства активируем графическую кносоль RabbitMQ, она будет работать на порту 15672:

Для того чтобы получить доступ к в браузере обращаемся к 192.168.122.200:15672 (к моему большому удивлению проброс портов осуществлять не надо было). Для регистрации необходимо присвоить один из тэгов (management, policymaker, monitoring, administrator) созданному ользователю openstack, так как за пользователя guest зайти будет нелья, из-за того что он может коннектиться [**только через loopback интерфейс**](https://www.rabbitmq.com/access-control.html#:~:text=%22guest%22%20user%20can%20only%20connect%20from%20localhost). Я пока не знаю какой тег больше подойдет для пользователя openstack, когда узнаю нужный подчеркну выше. А пока побыстрому создал пользователя test для проверки работоспособности web интерфейса.

Все нормально работает, главное выставить тэг:

\4. 	Установка и настройка БД MariaDB и PyMySQL.

4.1. Итак мы наконец закончили с конфигруацией RabbitMQ, теперь можно приступить к конфигрурирования БД для использования сервисами Openstack. Качаем  MariaDB и клиентскую библиотеку PyMySQ с помощью одной команды!

4.2. Создаем файл **/etc/my.cnf.d/openstack.cnf** с конфигом mysqld и запускаем mariadb:

- [**bind-address**](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_bind_address:~:text=в%20MySQL) = 192.168.122.200 — ip-адрес контроллера БД
- [**default-storage-engine**](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_bind_address:~:text=Управление%20паролями) = innodb - механизм хранения для таблиц по умолчанию, в данном случае [**innodb**](https://animatika.ru/info/gloss/innodb.html) (данные хранятся в больших совместно используемых файлах).
- [**innodb_file_per_table**](https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_file_per_table:~:text=WITH_DEBUG%20CMake%20option.-,innodb_file_per_table,-Command%2DLine%20Format) = on — для каждой таблицы будет создаваться новый файл
- [**max_connections**](http://max_connections/) = 4096 — максимальное количество подкючений
- [**collation-server**](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_collation_server:~:text=the%20default%20database.-,collation_server,-Command%2DLine%20Format) = utf8\_general\_ci — тип сервера сарвнения и сортировки строк, [**utf8_general_ci**](https://webcache.googleusercontent.com/search?q=cache:5FVk_T2r3uoJ:https://dev.mysql.com/doc/refman/8.0/en/charset-unicode-sets.html&cd=2&hl=ru&ct=clnk&gl=ru#:~:text=Ü%20%3D%20Y%20%3C%20Ö-,_general_ci%20Versus%20_unicode_ci%20Collations,-For%20any%20Unicode) — дефолт для utf8.
- [**character-set-server**](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_collation_server:~:text=and%20error%20messages.-,character_set_server,-Command%2DLine%20Format) = utf8 — тип набора символов на сервере

4.3. Запускаем супер-пупер мега скрипт [**mysql_secure_installation**](https://mariadb.com/kb/en/mysql_secure_installation/), который поможет безопасно установить MariaDB (что в принципе видно из названия), во время скрипта можно:

- Установить пароль для учетных записей root.
- Удалить корневые учетные записи, доступные из-за пределов локального хоста.
- Удалить учетные записи анонимных пользователей.
- Удалить тестовую базу данных, к которой по умолчанию могут обращаться анонимные пользователи.

Я везде отвечал yes (возможно это была критическая ошибка).

Проверить работу сервиса можно так:

\5. Установка и настройка сервиса идентификации Keystone

5.1. Теперь можно и потихоньку начать настраивать сервисы Openstack, все начинается в Keystone. Что такое Keystone — это сервис идентификации Openstack, представляет собой централизованный каталог пользователей и сервисов, к которым они имеют доступ. Если хотите еще одно синонимическое описание то вот: Keystone - единая система аутентификации и авторизации облачной операционной системы.Сервис поддерживает следующие типы аутенфикации:

- Аутентификация по токенам
- Аутентификация при помощи пары имя пользователя/пароль
- AWS-совместимая аутентификация

Также Keystone хранит в себе список всех доступных сервисов в Openstack и реквизитов для обращения к их API (без привязки к пользователю), из чего следует, что и поднимать его нужно первым, ведь потом каждый последующий сервис надо в нем регистрировать.

Что такое Сервис, Пользователь и Токен и так понятно, а вот Проект что-то новое — это объединение Ресурсов (виртуальные машины, образы и т. д.), придуман чтобы контролировать права пользователей, чтобы они могли видеть и работать с ВМ только внутри одного проекта (изначально думал, что это типо Namespace, но есть еще Домен). Также в Openstack как и в Kubernetes Пользователи напрямую не имеют доступ к Ресурсам Проектов, а получают его через так называемые Роли (понятно для чего так придумано, чтоб можно было создавать кастомные права доступа и быстро их назначать пользователям). Думали все, а нет, еще есть Домены — объединения Проектов, уже полные аналоги Namespase. Ладно по теории вроде бы все теперь качаем.

5.2. Качаем пакеты openstack-keystone (сам Keystone) [**pythonopenstackclient**](https://github.com/openstack/python-openstackclient) (удобный CLI для работы с Openstack) [**httpd**](https://httpd.apache.org/) (http сервер) [**mod_wsgi**](https://modwsgi.readthedocs.io/en/master/) (модуль http сервера для связи сервера и прогрмаммой на python) с помощью yum. **Проблема** подкралась незаметно — пакеты openstack-keystone и pythonopenstackclient **не скачались**, но хоть фиксится просто во первых в файле **/etc/yum.repos.d/epel.repo** дизейблим все репозитории (enabled=0), а во-вторых я дефис забыл в названии пакета **python-openstackclient**.


5.3. Как уже говорилось, для каждого сервиса Opensatack необходима отдельная БД, которую надо создавать руками, если происходит установка каждого сервиса отдельно, так что создаем Keystone БД, также надо выдать привилегии, чтобы сервис мог нормально работать с БД.

5.4. В конфиг Keystone (**/etc/keystone/keystone.conf**) пишем путь к БД, можно с помощью crudini, можно с vi и тд.



5.5. Задаем формат и провайдер токенов генерируемых сервисом Keystone. Формат токенов на выбор:

- **UUID** - строка из 32 символов, которую удобно использовать в вызовах OpenStack API, например применяя команду curl. Круто - небольшой размер, не круто –  токен не содержит информации, достаточной для того, чтобы произвести локальную авторизацию (сервисы OpenStack каждый раз должны отправлять токен сервису Keystone, для того чтобы получить информацию, какие операции разрешены с этим токеном).
- **PKI** - содержат всю необходимую для локальной авторизации информацию и, кроме того, содержат в себе цифровую подпись и информацию об устаревании, а значит сервисы OpenStack могут локально кэшировать эти токены. Круто — нет теперь дудоса токенами, не круто — токены большие (могут быть больше 8 Кб), и некоторые сервисы не поддерживают HTTP-заголовки такого размера, и еще как с curl работать, когда у тебя заголовок на 8 Кб.
- **PKIz** — пытались пофиксить проблемы PKI, но ничего не вышло, и они ушли в небытие.
- **Fernet** - небольшого размера (до 255 символов), но содержат достаточно информации для локальной авторизации. Их не требуется синхронизировать между регионами, для них не нужна база данных (токены без сохранения состояния), и процесс генерации их быстрее, чем в первых двух реализациях. Дополнительным плюсом будет отсутствие необходимости настройки memcached (можно не кешировать, но по итогу все равно надо, если хочешь быть крутым). **Их и используем.**

Выставляем провайдера токенов в файле **/etc/keystone/keystone.conf** сами знаете как.



5.6. Инициализируем базу данных (скрипт **keystone-manage db\_sync**) репозитории ключей Fernet (2 команды -  keystone-manage **fernet\_setup** и  **keystone-manage credential\_setup**).

5.7. Конфигурируем Apache httpd (доменное имя сервера) - сервис идентификации для сетевого взаимодействия, путем редактирован файла **/etc/httpd/conf/httpd.conf** (без использования cruduini).

Также создаем и редактируем файл **/etc/httpd/conf.d/wsgi-keystone.conf** (это конфигурация модуля http сервера для связи сервера и прогрмаммой на python).

Ну чтож вот [**описание полей**](https://www.opennet.ru/docs/RUS/apache_dir/#Location):

- **Listen** — на каком порту работает
- **VirtualHost** — имя virtualhost (люое на таком-то порту)
- **WSGIDaemonProcess** — создать отдельные процессы демона, которым будет делегирован запуск приложений WSGI.
- **WSGIProcessGroup** — в какой группе процессов будет выполняться приложение WSGI.
- **WSGIScriptAlias** — помечает каталог как содержащий сценарии WSGI
- **WSGIApplicationGroup** — какой группе приложений принадлежит приложение WSGI.
- **WSGIPassAuthorization** - передаются ли заголовки авторизации HTTP в приложение WSGI.
- **LimitRequestBody**  -  максимальное количество байт, которое может быть в запросе.
- **IfVersion** — если версия Apache больше/меньше
- **ErrorLogFormat** — формат лога ошибки
- **ErrorLog** — файл сохраения логов
- **CustomLog** — файл сохранения логов доп хоста
- **Directory** — местонахождение проекта
- **Require** **all** **granted** — нет IP-адресов, заблокированных для доступа к сервису (новая версия)
- **Order** **allow,deny** — [**настройка**](https://habr.com/ru/post/81858/) системы контроля доступа
- **Allow from all** — нет IP-адресов, заблокированных для доступа к сервису (старая версия)
- **Alias** — сопоставление URL-адреса с путем файловой системы
- **Location** — внутри настройки управления доступа к URL
- **SetHandler** — просмотр определнного хендлера
- **Options** — разрешенные особенности сервера



Наконец-то запускаем веб-сервер:

5.8. Теперь инициализируем Keystone. Опять же есть два варината инициализации:

- Использовать команду keystonemanage bootstrap, которая выполнит инициализацию за нас (рекомендовано разработчиками, поле того как попробовал второй способ использовал этот):

Данная команда создаст за нас записи о сервисе Keystone, точки входа (admin, internal и public) в сервис Keystone и регион в котором расположен сервис Keystone, а также default домен, проект admin, пользователь admin и роль admin и соеденит их:

Измененный скрипт (создаем сами, PS1 — для красивого названия в консоли):

Созданные ресурсы:


- Воспользоваться авторизационным токеном (общий секрет между Keystone и другими сервисами, а также вход в админку без пароля), долго, но зато идеально для понимания процессов (мой выбор, который был изменен на первый способ, так как этот способ был [**вырезан**](https://docs.openstack.org/keystone/rocky/admin/identity-auth-token-middleware.html#:~:text=The%20admin_token%20option%20is%20deprecated%20and%20no%20longer%20used%20for%20configuring%20auth_token%20middleware.)):

Для начала надо сгенерировать токен с помощью OpenSSL и поместить его в файл конфига Keystone:

Неможко займемся оптимизацией и напишем скрипт для задания значений переменным окружения (OS\_TOCKEN — наш токен, OS\_URL — URL Keystone, OS\_IDENTITY\_API\_VERSION — версия API Keystone)

После попытки отправки запроса на сервер, было получено много различных ошибок.

5.9. После того как помучились с иницализацией создадим непривилегированного пользователя и все что для него надо (admin пользователь был создан автоматически).

Создаем проект demo:

Создаем пользователя demo с собственным email:

Создаем роль user:

Добавляем роль user в проекте demo пользователю demo:

Скрипт для входа за user ():

5.10. Создаем проект service для всех сервисов Openstack:

С Keystone пока закончили, теперь можно перечислить полезные команды openstack:

- **openstack user list** — список пользователей
- **openstack role list** — список ролей
- **openstack role assignment list** — список назначения ролей
- **openstack domain list** — список доменов
- **openstack volume list** — скписок томов
- **openstack server add volume имя\_ВМ имя\_тома** — подключение тома к ВМ
- **openstack server remove volume имя\_ВМ имя\_тома** — отключение тома от ВМ
- **openstack service list** — список сервисов
- **openstack project list** — список проектов
- **openstack catalog list** — список каталогов
- **openstack network agent list** — список сетевых агентов (dhcp, open vswitch и тд)
- **openstack compute service list** — список служб Nova
- **openstack host list** — список узлов
- **opensatck hypervisor show имя\_узла** — информация о гипервизоре узла
- **openstack endpoint list** — список конечных точек
- **opensatck image list** — список образов
- **openstack image save имя\_образа > путь\_куда\_сохранять** — скачать образ на локальеую машину
- **opensatck тип show имя\_объекта** — информация о конуретом объекте
- **openstack тип delete имя\_объекта** — удаление объекта
- **openstack service create --name имя\_сервиса —description "описание" тип\_сервиса** — создание сервиса
- **openstack endpoint create тип\_точки тип\_интерфейса url --region регион** — создание конечной тчки
- **openstack domain create --description "описание" имя\_домена** — создание домена
- **openstack project create --domain имя\_домена --description "описание" имя\_проекта** — создание проекта
- **openstack user create --domain имя\_домена --email почта\_пользователя --password пароль\_пользователя имя\_пользователя** — создание пользователя
- **openstack role create имя\_роли** — создание роли
- **openstack role add --project имя\_проект --user имя\_пользователя имя\_роли** — создание назначения роли (привязка роли к пользователю)
- **opensatck тип set --имя\_поля значение\_поля имя\_объекта** — изменение полей объекта
- **opensatck тип unset --имя\_поля значение\_поля имя\_объекта** — удаление поля объекта
- **openstack --debug** — полезный флаг для просмотра вызовов к Openstack API
- **openstack token issue** — запрос токена для аунтефикации
- **openstack keypair create имя\_пары > файл\_сохранения\_пары** — создание пары ssh ключей
- **source keystonerc\_adm** — запуск скриптов
- **env** — просмотр переменных окружения
- **/etc/имя\_сервиса/имя\_сервиса(или)службы.conf** — файлы конфигов
- **/var/log/имя\_сервиса/имя\_сервиса-имя\_службы.log** — файлы логов (некоторые тут **/var/log/httpd/\***)

\6. Установка и настройка сервиса хранения образов Glance

Перед началом хочется отмтить, что обращения к Openstack API начали проходить достаточно медленно, после перезагрузки машины, возможно я дал ВМ слишком мало системных ресурсов, а может так и должно быть, вобщем пугаться долгих ответов сервера не надо, ниче не умерло.

6.1. Чтож для чего нам нужен Glance? Glance - ведет каталог, регистрацирует и доставляет образы виртуальных машин (как известно образ представляет собой шаблон для ВМ, может быть просто ОС, а может и содержать предустановленные пакеты). НО Glance самостоятельно не хранит образы, а использует для этого систему хранения данных (Swift, Ceph или просто локально на узле), информация же об размере, формате, имени образа и т. д. хранится в БД. Поддерживабтся следующие форматы образов, еще есть образы контейнеров, но это уже другая история:

- **vhd** (Virtual Hard Disk) —  виртуальный жесткий диск от Microsoft
- **vmdk** (Virtual Machine Disk) — диск виртуальной машины от VNware
- **vdi** (Virtual Disk Image) — образ виртуального диска от VirtualBox(Oracle)
- **iso** (International Organization for Standardization) — образ оптического диска от International Organization for Standardization
- **qcow2** (QEMU Copy On Write 2) — образ QEMU
- **ami** — образ Amazon Machine
- **ari** — образ Amazon Ramdisk
- **aki** — образ Amazon Kernel
- **vhdx** — улучшенная версия vhd
- **raw** — диск неструктурируемого формата RAW

Стоит отметить, что данный сервис состоит из двух служб:

- **glance-api** – предоставляет доступ к REST API сервиса образов для поиска, хранения и получения образов;
- **glance-registry** – хранит, обрабатывает и предоставляет информацию. Непосредственно пользователи не взаимодействуют с этим сервисом, только сервисы Glance.

Вот красивая картинка компонентов Glance:

Что происходит когда мы отправляем запрос на создание ВМ — Nova (еще не созданный сервис по управлению виртуальными машинами и сетью) отправляет GET-запрос по адресу **http://путь-к-сервису-glance/images/идентификатор-образа**. В случае если образ найден, то glance-api возвращает URL, ссылающийся на образ. Nova передает ссылку драйверу гипервизора, который напрямую скачивает образ и запускает ВМ.

6.2. Теперь начинается практика, первым делом добавляем все необходимые компоненты в а\наш Keystone:

Создаем пользователя glance:



Присваиваем пользователю glance роль admin в проекте serivce:

Создаем сервис glance:

Cоздадем точки входа для сервиса glance:

6.3. Теперь устанавливаем Glance с помощью yum:

6.4. Создаем БД glance и выдаем на нее права:

6.5. Прописываем строку подключения к базе данных в файлах конфигов служб glance-api (**/etc/glance/glance-api.conf**) и glance-registry (**/etc/glance/glance-registry.conf**):

6.6. Завершаем настройку БД glance с помщью скрипта (db\_sync):

6.7. Все что касается БД glance настроено, теперь настроим сам службу glance-api через конфиг (**/etc/glance/glance-api.conf**):

Настраиваем тип сервис аунтефикации:

Насраиваем разрешенный ip отправителя запросов к серивсу Glance:

Настраиваем URL аунтефикации:

Настраиваем тип авторицазции:

Настравиваем имя домена в котором нахоится проект glance:

Настравиваем имя домена в котором нахоится пользователь glance:

Настриваем имя проекта в котором находится пользователь glance:

Настриваем имя пользователя:

Настраиваем пароь пользователя glance:

Настраивем тип хранения образов (локальная файловая система):

Настраиваем путь к папке, гдк будут хранится образы:

Также стоит отметить, что можно добавить несколько хранилищ образов с приоритетом (больше цифра - больше приоритет сохранения образа) по использованию, но я так не делал:

Поля файла glance-api.conf после исполнения команд (легко найти через vi, потом /техт, который надо найти):

6.8. Теперь настроим сам службу glance-registry через конфиг (**/etc/glance/glance-registry.conf**), те же самые названия полей, кроме секции glance\_store:


6.9. Настраиваем параметры подключения к сервису RabbitMQ для службы glance-api (**/etc/glance/glance-api.conf**):

Настриваем пароль пользователя RabbitMQ:

Настриваем имя пользователя RabbitMQ:

Настриваем имя хоста RabbitMQ:


6.10. Запускае сконфигурированные службы glance-api и glance-registry:

Папку для хранения образов нужно создать, а то glance-api не запустится (словил прикол, что  у glance-api не было разрешений на файл, в котором хранятся логи - **/var/log/glance/api.log**). Решение прикола с разешениями:


6.11. Итак мы все насторили и запустили, теперь можно затестить

Для начала скачаем образ CirrOS – минималистская операционная система с помшью wget:

Необходимо также установить qemu-img, а потом посмотреть информацию о скачанном образе:

Какую информацию qemu-img:

- **file format** – формат диска
- **virtual size** – размер диска виртуальной машины
- **disk size** – действительный размер файла
- **cluster\_size** – размер блока (кластера) qcow
- **format specific information** – специфичная для формата информация (**compat** — версия QEMU, l**azy refcounts** — если on, то отключен ввод вывод метаданыых, **refcount bits** — размер refcount table (но я не уверен), **corrupt** — искаженные данные или нет)

Далее надо скачать какую-нибудь утилиту автоматизированного создания образов, я выбрал Oz, которая использует заранее подготовленные файлы ответов неинтерактивной установки операционной системы. Если во время установки Keystone, были ошибки как и у меня и поэтому пришлось отключить epel repo, то его (и только его) надо опять [**включить**](#disable).

Прверяем, что сеть libvirtd (перед этим libvirtd надо врубить), используемая по умолчанию с помощью команды **virsh net-list**, если нет, то определяем ее и задаем автозапуск сети:

Врубаем libvirtd:

У меня после запуска libvirtd дефолтная сеть задалась автоматически:

Если у вас команда **virsh net-list** вывела что-то другое, то исполните две команды **net-define** (инициализация сети) и **net-autostart** (автостарт сети):

6.12. Теперь, после того как скачали oz и настроили дефолтную сеть, можно начать настраивать oz.

Сначала зададим в качестве типа образа формат qcow2 в файле конфиграции oz (**/etc/oz/ oz.cfg**):

Теперь выбираем TDL шаблон для создания образа, примеры TDL шаблонов находятся в папке **/usr/share/doc/oz-0.15.0/examples**. Я списал самый простой TDL шаблон, а именно тот в котором определяются только путь к дистрибутиву (**<install>**) и пароль пользователя root (**<rootpw>**):

Полезные секции кроме **<install>** и **<rootpw>:**

- **<packages>** - установка пакетов
- **<repositories>** - добавление репозиториев
- **<files>** - создание файлов
- **<commands>** - выполнение команд

После создание TDL шаблона можно создать образ с помощью oz-install (**-t —** через сколько секунд инсталлятор должен прервать установку, **-d** — показывает уровень сообщений об ошибках (2 — норм, 3 — подробно)). Создается очень долго:

Образ должен появится в папке /**var/lib/libvirt/images**, но он весит 11GB, так что я его так и не скачал (диск на виртуалке 11GB), дальше если хотите подготовить образ к использованию в Openstack надо с помошью **virt-sysprep** (надо скачать) убрать специфичную для конкретного экземпляра машины информацию, потом уменьшить размер образа (превратить в тонкий диск) при помощи **virt-sparsify**. Если надо отредачить уже готовый образ то можно скачать **guestfish**:

6.13. Наконец-то после того как мы поработали с образами сами, можно поработать с обаразами с помощью Openstack. Для начала надо добавить в оба рабочих файла **keystonerc\_adm** и **keystonerc\_usr** переменную среды, определяющую версию Glance API, с которой мы будем работать (OS\_IMAGE\_API\_VERSION=2):


Загружаем наш скачанный образ Cirros в Glance:

Используемые параметры:

**--file** — путь до файла с образом

**--disk-format** — формат образа диска

**--container-format** — формат контейнера (bare — не контейнер)

**--public** — образ доступен всем

Для вас с прошлой строчки прошла одна секунда, а для меня - два дня, так как команда на скрине не выполнялась из-за того, что Glance не мог авторизоваться в Keystone (я так думал), в результате оказалось, что Glance не мог отправить запрос на URL <http://controller.test.local:35357/v3/>, так как это доменное имя не было прописано в **/etc/hosts** (но ведь запросы к Keystone шли норамльно!!!), чтобы все работало надо каждое имя узла добавлять в **/etc/hosts** (зато я поработал с файлами логов keystone и glance, а также флагом --debug):

После того как образ был загружен, он должен появится в этой папке **/var/lib/glance/images/**:

На этом все, но можно добавить, что образы популярных ОС можно найти тут:

- Ubuntu: [**http://cloud-images.ubuntu.com/**](http://cloud-images.ubuntu.com/)
- Fedora: [**https://cloud.fedoraproject.org/**](https://cloud.fedoraproject.org/)
- Debian: [**http://cdimage.debian.org/cdimage/openstack/**](http://cdimage.debian.org/cdimage/openstack/)
- CentOS: [**http://cloud.centos.org/centos/7/**](http://cloud.centos.org/centos/7/)

\7. Установка и настройка сервиса блочного хранилища Cinder

7.1. Cinder придуман для того чтобы хранить модифицированные в ходе работы тома, созданных образов, зачем? Ну, например, когда мы создали ВМ с помощью сервиса Nova (потом установим) мы с ней работаем (что-то устанавливаем, изменяемм и тд), а что если узел на которм была создана ВМ выйдет из сторя, и нам нужно ее срочно восстановить, Glance у нас хранит только базовые образы и никак не поможет, вот для из-за таких ситуаций и придуман Cinder, который записывает все изменения тома на узлах, где установлен cinder-volume. Также в Cinder можно не только хранить тома, но и создавать их снимки в режиме «только для чтения», с помощью которых создавать новые тома, доступные на запись.

Архитекртуа Cinder:

Службы Cinder:

- **cinder-api** – точка входа для запросов в сервис по протоколу HTTP. Приняв запрос, сервис проверяет полномочия на выполнение запроса и переправляет запрос брокеру сообщений для доставки другим службам
- **cinder-scheduler** – сервис-планировщик принимает запросы от брокера сообщений и определяет, какой узел с сервисом openstack-cinder-volume должен обработать запрос
- **cinder-volume** – сервис отвечает за взаимодействие с бэкэндом – блочным устройством. Получает запросы от планировщика и транслирует непосредственно в хранилище. Cinder позволяет одновременно использовать несколько бэкэндов. При этом для каждого из них запускается свой openstack-cinder-volume. И при помощи параметров CapacityFilter и CapacityWeigher можно управлять тем, какой бэкэнд выберет планировщик
- **cinder-backup** – сервис отвечает за создание резервных копий томов в объектное хранилище

7.2. К этому пункту свободное место на control узле равнялось 1GB, так что надо увеличивать размер:

Первым делом вырубаем VM2 и увеличиваем размер диска qcow2 на 3GB:

Потом запускаем VM2:

Теперь с помощью **lsblk** проверяем, что появился диск **/dev/vda2**, а с помощью **pvs**, что на нем стоит система:

Скорее всего увеличить размер партиции можно и другим способом, но я побоялся эксперементировать и скачал утилиту growpart:

После этого увеличиваем размер раздела **/dev/vda2**:

И наконец увеличиваем раздел **/dev/mapper/centos-root**:

7.3. Теперь можно начать настройку окружения для сервиса Cinder, для этого надо добавть к виртуальной машине еще один диск в качестве блочного устройства, созданного с помощью  [**Linux LVM**](https://habr.com/ru/post/67283/) (испульзует протокол [**iSCSI**](https://community.fs.com/ru/blog/iscsi-storage-basics-plan-iscsi-san.html)), на который будут писаться все изменения томов ВМ паралельно их записи на локальную систему рабочего узла.

Создадим новый диск и прикрепим его к запущенной VM2:

В VM2 проверяем присоедененный диск:

Теперь создаем LVM-группу на присоедененном диске:


7.3. Теперь качаем пакет openstack-cinder и редачим его конфигурационный файл **/etc/cinder/cinder.conf**.

Настриваем подключение к БД:

Настраиваем URL, имя и пароль RabbitMQ:

Как раньше настриваем реквизиты пользователя и проекта в Keystone:

Настройкв имени lvm-группы:

Настриваем бэкенд хранения Cinder, а также драйвер (отвечает за хранение данных при помощи локального менеджера логических томов и протокола транспорта iSCSI) для него:

Настриваем использемый протокол транспорта и возможность использовать команду cinder-rtstool для управления томами (lioadm — поддержка [**LIO iSCSI**](https://habr.com/ru/post/200466/#:~:text=Встречайте%2C%20вот%20они.-,LIO,-linux%2Discsi.org)):

Настройка URL Glance API:

Настройка пути к папке, где будут хранится [**lock-файлы](https://docs.openstack.org/oslo.concurrency/ussuri/admin/index.html)** (файлы, создаваемые сервисами для межпроцессорной блокировки):

7.4. Теперь аналгочно прошлым настройка создаем БД Cinder:

Во время выполнения скрипта синхронизации вылезают сообщения об устаревших полях, полностью их игнорим, БД и так поднимется:

7.5. Теперь регистрируем сервис Cinder в Keystone:

Создаем пользователя и предоставлем ему роль:

Создаем сервисы cinderv2 и cinderv3. Раньше надо было иметь две версии cinder для полной совместимости с остальными серивсам, но вроде как это уже [**не необходимо**](https://docs.openstack.org/cinder/latest/install/cinder-controller-install-rdo.html#:~:text=Beginning%20with%20the%20Xena%20release) и можно ограничится только сервисом cinderv3 (я боюсь и разверну 2 сервиса):

Соответсвенно раз серивиса два, то и точек входа в два раза больше:



7.6. Теперь устанавливаем и настраиваем сервис iSCSI, по идее следующие действи я надо производить на других узлах для большей доступности и отказоустойчивости, пожтому некоторые команды могут повторяться.

Устанавливаем [**targetcli**](https://www.saqwel.ru/articles/linux/nastrojka-linux-iscsi-posredstvom-targetcli/) (оболочка для управления LIO iSCSI):

Задаем ip машины на которой будет находится cinder-volume:

Задаем бэкенд хранения Cinder:

По итогу всех махинаций наш конфиг (**/etc/cinder/cinder.conf**) выглядит так:

7.7. Теперь запускаем все что можно:





Также все эти службы можно было запустить одной командой с помощью пакета [**openstack-utils**](https://github.com/redhat-openstack/openstack-utils), который надо скачать, так как с помощью него также можно удобно селдить за статусами всех сервисов кластера:

Запуск всех служб Cinder одной командой:

Просмотр статуса всех сервисов одной командой (если что keystone и должен быть в inactive, мы ведь его напрямую не поднимали, а через RabbitMQ):

Также можно посмотреть состояние служб Cinder (cinder-backup по идее должен был быть оcтановлен, так как ему не задана конечная точка бэкенда хранения этих самых бэкапов (то самый сервис Swift из будущего), он почему-то работет, но в логах пишет, что могут быть проблемы):

Содержимое файла логов **/var/log/cinder/backup.log** cinder-backup:

На этом настройка закончена, теперь тестим функционал. Для этого попробуем создать том (volume) testvol1:

Пока с помощью openctack cli посмотреть на созданный том (**openstack volume list**) нельзя, так как мы еще не развернули сервис nova, но можно посмотреть с помошью cinder cli:

Зря я сделал диск размером всего в 2GB:

7.8. По идее на этом можно закнчивать настройку сервиса Cinder, но надо кое-что уточнить, а именно, что в реальной жизни бэкендом cinder-volume и cinder-backup должен быть сервис Swift или Ceph, которые нужно разворачивать на других узлах. Мой ноут с 8GB оперативки и в перспективе двумя рабочими узлами такое точно не потянет, но все-таки было принято решение сделать бэкендом cinder-backup nfs сервер расположенный на одном из рабочих узлов в качестве эксперимента.

Итак для начала нужно этот рабочий узел поднять (когда я в первом пункте сказал, что поднял 3 ВМ, я соврал):


Теперь наконец-то понятная настройка ВМ для работы с Openstack (если что файл **/etc/hosts** дожен выглядить да всех ВМ одиноково, так что редачим его на кождой ВМ):


Теперь настраиваем все что касается [**nfs сервера**](https://winitpro.ru/index.php/2020/06/05/ustanovka-nastrojka-nfs-servera-i-klienta-linux/):

Скачиваем и запускаем nfs-server:

Создаем папку в которая будет общая для всех nfs-клиентов;

Редактируем файл **/etc/exports**, указывая каким ip-фдресам будет доступна папка, после чего применяем настройка и перезапускаем nfs-сервер:

- **rw** – права на запись в каталоге (ro – доступ только на чтение)
- **sync** – синхронный режим доступа (async – подразумевает, что не нужно ожидать подтверждения записи на диск)
- **no\_root\_squash** – разрешает root пользователю с клиента получать доступ к NFS каталогу (обычно не рекомендуется)
- **no\_all\_squash** — включение авторизации пользователя (all\_squash – доступ к ресурсам от анонимного пользователя)

Теперь попробуем настроить NFS бэкенд для cinder-backup на **controllet.test.local** (оказалось, что не обязательно настривать nfs-клиента самостояетльно, монтироватьпапки и тд, достаточно просто добавить апарметры в конфиг Cinder **/etc/cinder/cinder.conf**, и все смонтируется автоматически после перезагрузки сервиса Cinder):

Настраиваем драйвер для cinder-backup и папку nfs-сервера, которую надо смонтировать:



Все, поздравляю Swift можно не настраивать как и Ceph, но сами понимаете, что если мощность позволяют надо боязательно ставить или Swift или Ceph, желательно на трех отдельных узлах. Пока нет желания про них что-то расписывать, может только в самом конце, когда все будет настроено, я раверну Swift или Ceph на рабочих узлах.

\8. Установка и настройка сервиса управления виртуальными машинами и сетью Nova

8.1. Сервис Nova у нас отвечает за за управление запущенными экземплярами виртуальных машин, а также сетью (раньше да, теперь за сеть отвечает Neutron) как видно из названия раздела. Состоит из следующих служб (помимо RabbitMQ (брокера сообщений) и собственной БД):

- **openstack-nova-api** – как и подобные службы других рассмотренных сервисов, отвечает за обработку пользовательских вызовов API.
- **openstack-nova-scheduler** – сервис-планировщик. Получает из очереди запросы на запуск виртуальных машин и выбирает узел для их запуска. Выбор осуществляется, согласно весам узлов после применения фильтров (необходимый объем оперативной памяти, определенная зона доступности и т. д.). Вес рассчитывается каждый раз при запуске или миграции виртуальной машины.
- **openstack-nova-conductor** – служба выступает в качестве посредника между базой данных и nova-compute, позволяя осуществлять горизонтальное масштабирование (нельзя развертывать на тех же узлах, что и nova-compute).
- **openstack-nova-novncproxy** – выступает в роли [**VNCпрокси**](https://habr.com/ru/post/76237/) (такую опицю можно было выбрать во время запуска виртуалок при помощи **virsh**) и позволяет подключаться к консоли виртуальных машин при помощи браузера.
- **openstack-nova-consoleauth** – отвечает за авторизацию для сервиса openstack-nova-novncproxy (вроде как больше не поддреживется).
- **openstack-nova-placement-api** – сервис отвечает за отслеживание списка ресурсов и их использование (рагьше был частью nova, потом отпочковался и стал назваться **opensatck-placement-api**, а по-благородному **Placement**).
- **openstack-nova-compute** – демон, управляющий виртуальными машинами через API гипервизора. Как правило, запускается на узлах, где располагается сам гипервизор.

Все сервисы кроме nova-compute располгагаются на controller узле, а nova-compute на рабочих узлах comupte и compute-opt (пока не создан).

8.2. Приступаем к установке сервиса Nova:

Как всегда базовые начальные шаги (и хорошо, что они у всех сервисов очень похожие):

Устанавливаем все службы Nova, кроме **openstack-nova-compute** (Placement раз уж он был когда-то частью Nova тоже скачаем и потом настроим, также возможно не надо качать **opemsatck-nova-console**, так как **openstack-nova-consoleauth** больше не поддреживается, но это не точно):

Создаем три бд (**nova** — содержит всю информацию о ВМ, которые удалось запланировать, **nova-api** — содержит информацию о местонахождении и временном (еще только создаются) местонахождении ВМ и **nova-cell0** — содержит ВМ, которые не удалось запланировать) и выдаем всем привилегии:

Регистрируем сервис Nova в Keystone:


Теперь регистрируем сервис Placement в Keystone:


Далее конфигурируем файл **/etc/nova/nova.conf** (все параметры (я напишу какие не надо) ниже также надо будет добавить в аналогичные файлы на рабочих узлах):

Базовая конфигурация (**enabled\_apis** — разрешенные API):

Теперь на будущее включаем поддержку сервиса Neutron (**firewall\_driver** — должно быть такое занчение, если используется **Neutron** вместо **nova-network**):

Настраиваем URL для использования сервиса Glance:

Указываем имя региона для сервиса Cinder:

Указываем путь к фалам блокировок:

Настраиваем параметры для подключения к сервису Placement:


На этом общие параметры заканчиваются и начинаются специфичные для **controller.test.local**:

Настраиваем пути к созданным БД (почему только к 2 из 3 я без понятия):

Указываем ip **controller.test.local**:

Указыаем ip адрес на которм будет работать VNC-сервер:


Теперь можно инициализировать базу данных nova-api:

Также надо зарегестрировать базу данных cell0 в БД nova и создать ячейку cell1:

Теперь заполняем (очень долго) БД nova (я уже сошел с нити понимания конфигурации):

8.3. Перед тем как запустить все сервисы Nova сконфигурируем и запустим сервис Placement (оказывается зарегестировать его в Keystone было недостаточно):

Для этого создадим БД placement и выдадим рарешения:

Так-как в Keystone мы placement зарегестрировали, пропускаем этот шаг и заполняем файл кнфига **/etc/placement/placement.conf**:


Синхронизируем БД placement:

8.4. Запускаем все влужбы Nova и презапускаем httpd (насчет **openstack-nova-console** пока не знаю, поэтому не запускаю, так как в офицальном гайде Opensatck (не из книги) **openstack-nova-console** даже не скачивается):


8.5. На этом настройка сервиса Nova на **controller.test.local** закончена и можно переходить к настройке рабочих узлов (все настройки проводятся на **compute.test.local**, **compute-opt.test.loca**l аналогично, но с другими ip).

Первым делом устнавливаем пакеты **openstack-nova-compute** (служба Nova для рабочих узлов) **sysfsutils** (утилита получения информации о ВМ) **libvirt** (утилита виртуализации), если не качается фикс [**здесь**](#disable):

Далее необходимо проверить поддержку аппаратной виртуализации на рабочем узле (**svm** — поддержка от Amd, **vmx** — от Intel):

Если вдруг аппаратная виртуализация не поддерживается (пустой вывод команды выше), то необходимо включить эмуляцию с помощью QEMU:

Теперь настраиваем конфигурационный файл сервиса Nova **/etc/nova/nova.conf**, для начала базовые параметры для всех узлов, о которых упоминалось [**ранее**](#work):

Теперь устанавливаем параметры **/etc/nova/nova.conf** специфичные для рабочих узлов:

Указываем адрес вычислительного узла (**compute-opt** — 192.168.122.215):

Включаем поддрежку vnc, а также указываем что VNC-сервер будет слушать подключения на всех интерфейсах и URL прокси-сервера, где будет доступен браузеру интерфейс виртуальной машины:

Указываем адрес узла-клиента nvcproxy (**compute-opt** — 192.168.122.215):

На этом работа с конфигом заканчивается и можно запускать сревисы **libvirtd** **openstack-nova-compute** (**openstack-nova-compute** не запустится, если **controller** узел не в сети):

8.6. Теперь проверяем что мы там настроили и заканчиваем синзронизацию на **controller** узле:

Проверяем видит ли controller узел наш compute узел:

Если видит, то регистрируем гипервизоры с помощью скрипта (вывод примерно такой, честно скажу, свой вывод для **compute.test.local** я потерял, так что взял вывод после подключения **compute-opt.test.local**):

Теперь можно проверить работу Placement API с помощью еще одного скрипта:

**Если скрипт выводит ошибку 403** — то вы такой же как и я (хотя как может быть по другому, если вы шли по этому гайду), так вот ошибка решается добавлением секции **<Directory /usr/bin>** в конфигурационный файл httpd для Placement API **/etc/httpd/conf.d/00-placement-api.conf** и ппоследующим перезапуском сервиса httpd:

На этом настройка сервиса Nova закончилась (нашел [**крутой современный гайд**](https://www.informaticar.net/openstack-compute-installation-tutorial-centos/) на установку Openstack), теперь экспромтом покажу как настроить **nfs-клиента** на **compute-opt** узле, так как автоматически он его не создаст:

Во-первых вот так по итогу выглядит итоговый **/etc/hosts**, каждого узла нашего проекта:

Скачиваем и запускаем nfs-server:

Создаем папку, в которую мы смонтируем nfs-каталог, и собственно монтируем в нее nfs-каталог нашего nfs-сервера:

Для того чтобы каталог автоматически монтировался, добавим соответствущую строку в файл **/etc/fstab** и применим конфигурацию:

- **rw** — разрешение на чтение и запись
- **sync** — все изменения сразу сбрасываются на диск без использования кэша
- **hard** — клиент будет пинговать nfs-сервер, если тот станет недоступен
- **intr** — монтирование можно прервать

\9. Установка и настройка сетевого сервиса Neutron

9.1. Про сервис Neutron написано много, так что введение будет большим. Для начала Neutron — реализует коммутацию (NetFlow, RSPAN, SPAN, LACP, 802.1q, туннелирование GRE и VXLAN), балансировку нагрузки, брандмауэр и VPN в кластере Opensatck (раз он сетевой сервис). Определим основные абстракции сервиса:

- **сеть** – есть внутренние, виртуальные сети, которых может быть много, и как минимум одна внешняя. Доступ к виртуальным машинам внутри внутренней сети могут получить только машины в этой же сети или узлы, связанные через виртуальные маршрутизаторы. Внешняя сеть представляет собой отображение части реально существующей физической сети для обеспечения сетевой связанности виртуальных машин внутри облака и «внешнего мира».
- **подсеть** – сеть должна иметь ассоциированные с ней подсети. Именно через подсеть задается конкретный диапазон IP-адресов
- **маршрутизатор** – как и в физическом мире, служит для маршрутизации между сетями.
- **группа безопасности** – набор правил брандмауэра, применяющихся к виртуальным машинам в этой группе
- **«плавающий IP-адрес»** (Floating IP) – IP-адрес внешней сети, назначаемый экземпляру виртуальной машины. Он может быть выделен только из существующей внешней сети (по умолчанию каждый проект имеет квоту в 50 адресов)
- **порт** – подключение к подсети. Порт на виртуальном коммутаторе. Включает в себя MAC-адрес и IP-адрес

Соответсвенно в кластере приходится иметь дело со следующими видами трафика:

- **внешний трафик** — публичная сеть, которой принадлежат «реальные» IP-адреса виртуальных машин, а также демилитраизированная зона
- **внутренний трафик** — сеть для операционных систем и служб (ssh и мониторинг), сеть для сетевой установки узлов под сервисы OpenStack, сеть для IPMI, DRAC, iLO, консолей коммутаторов и сеть передачи данных для служб SDS (Swift, Ceph, iSCSI, NFS)
- **трафик виртуальных машин** — сеть свзяи внешнего трафика с ВМ
- **трафик API и управления** — сеть, предоставляющая наружу публичный API и веб-интерфейс Horizon

Для функционирования Neutron нужно минимум три узла:

- **узел управления** — узел с борокером сообщений (**controller.test.local**)
- **сетевой узел** — **производительный** физический сервер или физический коммутатор
- **вычислительный узел** — некоторые службы Neutron добавляются на каждый рабочий узел (**compute.test.local** и **compute-opt.test.local**)

Почему все используют Neutron вместо nova-network? Потому что в Neutron можно расширять API при помощи подключаемых модулей (в nova-network для настройки сети использовались только возможности ОС на котрой работает узел), при этом у Neutron есть супер модуль **Modular Layer 2** (ML2 – OVS/LB), позволяющий использовать сразу несколько технологий 2 уровня (раньше можно было только LinuxBridge или только Open vSwitch) путем использования специальных драйверов (присутсвуют из коробки):

- драйвера агентов, например LinuxBridge или OVS
- драйвера контроллеров SDN, например OpenDaylight, OpenContrail, VMware NSX или PLUMgrid
- драйвера аппаратных коммутаторов, например Cisco Nexus или Extreme Networks

Также можно качать сторонние [**драйвера](https://wiki.openstack.org/wiki/Neutron_Plugins_and_Drivers)** или разворачивать сервисы:

- **маршрутизатор**
- **балансировщик** **нагрузки** (LBaaS)
- **брандмауэр** (FaaS)
- **виртуальные** **частные** **сети** (VPNaaS)

Наконец на 65 старнице можно чатично понять смыл проектов Openstack — каждый проект может иметь свою сеть, соответсвенно один узел может иметь несколько адресных пространств которые даже могут пересекаться (несколько ARP таблиц и таблиц маршрутизации, наборов правил брандмауэра, сетевых устройств и т. д).

Что касается служб Neutron:

- **neutron-server** – центральный управляющий компонент. Не занимается непосредственно маршрутизацией пакетов. С остальными компонентами взаимодействует через брокер сообщений **(сидит на узле управления)**
- **neutron-openvswitch-agent** – взаимодействует с neutron-server через брокер сообщений и отдает команды OVS для построения таблицы потоков **(сидит на сетевом и рабочих узлах)**
- **neutron-l3-agent** – обеспечивает маршрутизацию и NAT, используя технологию сетевых пространств имен **(сидит на сетевом узле)**
- **openvswitch** – программный коммутатор, используемый для построения сетей **(сидит на сетевом и рабоих узлах)**
- **neutron-dhcp-agent** – сервис отвечает за запуск и управление процессами dnsmasq, легковесного dhcp-сервера и сервиса кэширования DNS. Также neutrondhcp-agent отвечает за запуск прокси-процессов сервера предоставления метаданных (каждая сеть, создаваемая агентом, получает собственное пространство имен qdhcp-UUID\_сети) **(сидит на сетевом узле)**
- **neutron-metadata-agent** – данный сервис позволяет виртуальным машинам запрашивать данные о себе, такие как имя узла, открытый ssh-ключ для аутентификации и др (ВМ получают эту информацию во время загрузки скриптом, обращаясь на адрес http://169.254.169.254. Агент проксирует соответствующие запросы к openstack-nova-api при помощи пространства имен маршрутизатора или DHCP) **(сидит на сетевом узле)**
- **neutron-ovs-cleanup** – отвечает во время старта за удаление из базы данных OVS неиспользуемых мостов «старых» виртуальных машин **(сидит на сетевом узле)**

Итак что происхдит в **Neutron** при создании ВМ через сервис **Nova**:

1. Сервис **Nova** отправляет запрос на сервис neutron-server, отвечающий за API
1. **Neutron-server** отправляет запрос агенту DHCP (**neutron-dhcp-agent**) на создание IP-адреса
1. **Neutron-dhcp-agent** обращается к сервису **dnsmasq**, отвечающему за подсеть, в которой создается виртуальная машина
1. **Dnsmasq** возвращает первый свободный IP-адрес из диапазона адресов подсети
1. **Neutron-dhcp-agent** отправляет этот адрес сервису **neutron-server**
1. После того как за виртуальной машиной закреплен IP-адрес, сервис **Neutron** отправляет запрос на **Open vSwitch** для создания конфигурации, включающей IP-адрес в существующую сеть.
1. **Open vSwitch** возвращает обратно параметры конфигурации на сервис **Neutron** при помощи шины сообщений
1. **Neutron** отправляет параметры конфигурации сервису **Nova**
1. Конец

Небольшая сноска перед следующим пунктом, я тут нешел рекомендуемые требования к узлам… (у меня всего 8GB, возможно в скором времени мне это все еще как аукнется, как минимум Swift и Ceph даже если бы хотел развернуть не смог), в такие моменты внезапно понимаешь, что по идее эти узлы должны одновременно запущенны 24/7 и их не надо останавливать, чтобы перестать ловить фризы всей системы, а ведь в переди еще как минимум 3 сервиса.

9.2. Начинаем кстановку и настройку сервиса Neutron на узле управления (**controller.test.local**):

На всякий случай сделаем снимок состояния виртуалки, а то мне кажется,что грань великих тормозов все ближе:

Устанавливаем пакеты **openstack-neutron** (службы **Neutron**) **openstack-neutron-ml2** (тот самый [**волшебный модуль**](#ML2)):

Классическое создание БД для сервиса, на это раз как нетрудно понять БД зовется neutron:

Далее… да вы угадали — регистрируем сервис в Keystone:


Далее идут общие параметры конфигурационных файлов для сетевого, рабочих и управляющего узлов. Neutron — король по количеству настроек, вот описание файлов конфигураций, которые мы будем изменять:

- **/etc/neutron/neutron.conf** — основной конфиг Neutron
- **/etc/neutron/plugins/ml2/ml2\_conf.ini —** конфиг модуля ML2
- **/etc/neutron/plugin.ini —** сслыка на конфиг ML2, там прикол, что если не будет ссылки то конфиг ML2 не подключится, так как находится в другой папке.
- **/etc/neutron/l3\_agent.ini** — конфиг L3-агента (отвечает за маршрутизацию)
- **/etc/neutron/dhcp\_agent.ini** — конфиг DHCP-агента
- **/etc/neutron/metadata\_agent.ini** — конфиг агента, предоставляющего метаданные виртуальным машинам
- **/etc/neutron/plugins/ml2/openvswitch\_agent.ini** — конфиг агента Open vSwitch для модуля ML2

Ну шо приступаем к настройке **/etc/neutron/neutron.conf**, первым делом настраиваем баозовые вещи (подключение к броекру сообщений и аунтефикацию Keystone):

Указываем назваоние основного подлкючаемого модуля второго уровня модели OSI (также есть : hyperv, cisco, brocade, embrance, vmware, nec и др):

Указываем погружаемые модули (из router, firewall, lbaas, vpnaas, metering), подгружаем только маршрутизатор:

Настройка разрешения пересечения IP внутри проектов:

Забыл настроить путь к файлам блокировки:

Настройка **/etc/neutron/neutron.conf** закончена (вроде как), теперь настраиваем **/etc/neutron/plugins/ml2/ml2\_conf.ini**:

Настраиваем используемые сегменты сетей (выбр из: flat, GRE, VLAN, VXLAN и local):

Настраиваем технологии используемые для сетей проектов (поддерживающие пересечение IP: GRE, VLAN, VXLAN и flat):

Настраиваем тип драйвера механизма ML2 (выбор из: openvswitch, mlnx, arista, cisco, logger, linuxbridge и brocade):

Настраиваем диапазон ID для [**GRE-туннелей**](https://wiki.dieg.info/gre):

Включаем группы безопасности [**ipset**](https://www.dmosk.ru/terminus.php?object=ipset):


Выше были настройки общие для всех узлов, теперь пойдут настройки только для управляющего узла:

Настраиваем параметры поключения к БД:

Настраиваем отправку уведомлений сервису Nova при изменении статуса портов и IP-адресов:

Настраиваем параметры пподключения к Keystone сервиса Nova (видимо Neutron нужно в определенных обстоятельствах нужно выдавать себя за Nova):

Переносим все заботы о сети с сервиса Nova на Neutron  в файле **/etc/nova/nova.conf** (еще брандмауэр Nova вырубаем), тетью команду отдельно выносить не буду, там мы задаем поддержку Open vSwitch:

Настраиваем  параметры аутентификации Neutron для Nova, если раньше мы писали их в **/etc/neutron/neutron.conf**, то теперь в **/etc/nova/nova.conf**:

Теперь можно синхронизировать БД:

У меня вылезла ошибка **«No module named MySQLdb»**, почему только сейчас известно только разрабам Openstack (хотя вывод скрипта огромный, мб он единственный использует это модуль), но фиксится она как нетрудно догадаться скачивание данного модуля (после снапшота, я теперь готов все что угодно качать):

Теперь создаем ссылку на конфиг ML2 в папке настроек Neutron **/etc/neutron/**:

Указываем использование прокси для сервера метаданных и общий секрет, которым будут подписываться запросы к серверу метаданных в конфиге **/etc/nova/nova.conf:**

По итогу на узле управления в конфиги добавляются следующие параметры:

Перезапускаем сервисы **nova** и стартуем сервис **neutron-server**:

Проверка итога настройки:

9.3. Теперь я оказался на перепутье ведь надо настраивать сетевой узел как отдельный узел, а нутбук уже просит пощады, так что мой выбор пал на разворачивание сетевого узла на узле управления, может выйти гениально, а может и нет (но я ведь не зря снимок сделал). Далее я буду писать как будто настраиваю отдельный узел, но делать пометки лишних действий, так как некоторые уже дела при настройке узла управления:

Начнем с утановки необходимых пакетов (если как и я устанавливаете на узел упраления, то **openstack-neutron** и **openstack-neutron-ml2** качать не надо) **openstack-neutron-openvswitch** (модуль openvswitch для openstack) и [**openvswitch](https://habr.com/ru/post/325560/)** (программный коммутатор):

Далее идет установка всех общих параметров конфигов (на узле управления уже установлены) и создание ссылки на конфиг в **/etc/neutron/plugins/ml2/ml2\_conf.ini**:


Дальше уже страшно, так как нужно внести именения в параметры ядра, а именно включить маршрутизацию пакетов (**net.ipv4.ip\_forward**) и выключить фильтрацию пакетов по их исходящему адресу (**net.ipv4.conf.all.rp\_filter** и **net.ipv4.conf.default.rp\_filter**):

Настраиваем конфиг ML2 **/etc/neutron/plugins/ml2/ml2\_conf.ini**, указывая **[flat**](https://wiki5.ru/wiki/Flat_network)** провайдер для внешней сети:

Настраиваем конфиг Open vSwitch **/etc/neutron/plugins/ml2/openvswitch\_agent.ini**:

Указываем IP-адрес для конечной точки туннеля (ip узла управления):

Сопоставляем внешнюю сеть созданному мосту br-ex:

Настраиваем включение GRE-туннелирования:

Настраиваем конфиг L3-агента (маршрутизация) **/etc/neutron/l3\_agent.ini**:

Указываем драйвер Open vSwitch:

Настриваем использование namespace:

Указываем имя, используемого моста:

Настриваем удаление неиспользуемых namespace:

Настраиваем конфиг DHCP-агента **/etc/neutron/dhcp\_agent.ini**:

Указываем драйвер Open vSwitch:

Указываем Dnsmask DHCP драйвер:

Настраиваем успользование и удаление namespace:

Настриваем конфиг агента-метаданных **/etc/neutron/metadata\_agent.ini**:

Указывае IP-адрес управляюшего узла:

Указываем общий секрет, используемый для получения метаданных;

Дополнительные параметры конфигов сетевого (управляющего) узла:

Запускаем и включаем Open vSwitch:

Я тут понял, что в определенный момент перестал показывать скрины статусов, но не переживайте, все работает без ошибок.

Ухх, господа, теперь [**надо создать сетевой интерфейс**](https://bozza.ru/art-266.html) для нашей так называемой внутренней сети (в книге он был с первой главы, в то время как у меня был только eth0 на всез ВМ), так что выходим из нашей виртуалки на хост и начинаем создавать новую сеть типа bridge:

Первым делом создаем файл конфигурации сети **private.xml** (я просто скопировал содержимое файла **/etc/libvirt/qemu/networks/default.xml** и поменял ip и удалил части, которые сгенерятся автоматически), потому что virsh почему-то не может создать сеть через параметры командной строки:

Создаем сеть с помощью virsh:

Включаем автострат сети private (перед настройкой автостарта, нужно открыть конфиг сети через **net-edit**, чтобы первоначальные настройки сети применились):

Должен появится сетевой интерфейс **virbr3** на хосте:

Теперь можно запускать VM2 (**controller.test.local**) и присоеденять к ней сетевой интерфейс (команда выполнятеся на хосте):

Внутри ВМ должен появится новый интерфейс **eth1**:

На этом все, сетевой интерфейс к машине присоединен, теперь можно перейти к его настройке для сетевого (у меня управляющего) узла для сервиса Neutron:

Создаем файл **/etc/sysconfig/network-scripts/ifcfg-eth1** и пишем в него следующие настройки и перезапускаем сеть:

Так теперь выглядит eth1 интерфейс в выводе команды **ip**:

Теперь надо добавить мост **br-ex** для Open vSwitch, который будет находится на созданном интерфейсе **etn1**:

Проверяем все ли подключилось нормально при помощи **ovs-vsctl**:

Вроде все круто, так что можно запускать все службы Neutron для сетевого (сами знаете какого) узла (насколько я понял ovscleanup стартовать не надо):

Вместо того, чтобы проверять статус каждой службы можно посмотреть все одной командой:

Теперь переходим к настройке служб Neutron на рабочих узлах **compute** и **compute-opt**:

Качаем необходимые пакеты (все уже обсуждались раньше):

Теперь заполняем общие для всех узлов параметры конфигов для сервиса Neutron, и создаем ссылку на конфиг ML2:


Базу выполнили, теперь изменяем параметры ядра **net.ipv4.conf.all.rp\_filter**  и **net.ipv4.conf.default.rp\_filter** (отключение фильтрации пакетов), **net.bridge.bridge-nf-call-iptables** (пакеты с сетевого моста передаются на обработку iptables):



Если последняя строка не вполняется то выполните следующую команду (включает модуль ядра [**br_netfilter**](https://ebtables.netfilter.org/documentation/bridge-nf.html)), а потом опять **sysctl**:

Так, ну теперь пошли специфичные для рабочих узлов параметры конфигов:

Настраиваем конфиг Open vSwitch **/etc/neutron/plugins/ml2/openvswitch\_agent.ini**:

Указываем локальный ip вычислительного узла (**compute-opt** — 192.168.122.215):

Указываем тип используемой технологии тунеля:

Указываем драйвер брандмауэра (iptables на open vSwitch):

Запускаем Open vSwitch, так как его настройка закончена:

Настраиваем конфиг сервиса Nova **/etc/nova/nova.conf**:

Передаем полномочия (первые 3 команды) с службы **nova-network** на **Neutron**, а также отключаем службу брандмауэра в **Nova** (последняя команда):

Указываем параметры аунтефикации Neutron (как и на управляющем узле):

Персональные папрметры рабочих узлов:

Перезапускаем службу **nova-compute** и запускаем агента Open vSwitch:

Должны добавится новые сетевые агенты в выводе команды, по одному на рабочий узел (я пока настроил только один):

Все, с сервисом Neutron пока покончено, как и с основными (точнее 6 из 7, без Swift) сервисами Opensatck. Осталось только поработать с созданием ВМ и сетей для них, а также установить доп сервисы для удобства (Horizon, Gnocchi. Ceilometr, Aodh и Heat). Есть также некоторые сомнения касательно подключения дополнительных сетевых интерфейсов для рабочих узлов (пока не подключаю), если их надо будет добавить, естественно, сделаю сноску.

\10. Тестим все то, что наразворачивали.

10.1. Для начала создадим все необходимые компоненты сетевой инфраструктуры для запуска первой ВМ:

Создаем внешнюю сеть **ext-net**:

Из-за опечатки в книге (вместо --provider-physical-network external было написано --provider-physical-network datacentre) я потратил очередные 2 часа своей жизни на исправление бага… А ведь это уже 4 издание.

- **--external** — сеть является внешней (есть средство внешней маршрутизации)
- **--share** — сеть доступна всем проектам
- **--provider-network-type** — физический механизм, с помощью которого реализуется виртуальная сеть (flat, geneve, gre, local, vlan, vxlan)
- **--provider-physical-network** — имя физической сети, в которой реализована виртуальная сеть (мы мапили это название с созданным мостом, а потом объявляли его возможным для использования в типе сети flat)

Теперь создадим подсеть, из которой будут выделяться плавающие IP-адреса (в реальности они должны быть видны из интернета).

- **--network** — сеть родитель
- **--no-dhcp** — без DHCP
- **--allocation-pool** — доступные ip адреса
- **--gateway** — дефолтный шлюз во внешюю сеть (ip маршрутизатора)
- **--subnet-range** — маска подсети

Теперь переквалифицируемся в пользователя demo и создаем сеть в его личном проекте:

Создаем подсеть (**demo-subnet**) нашей **demo-net**:

Создаем роутер (**demo-router**) для **demo-subnet**:

Добавляем подсеть **demo-subnet** к роутеру **demo-router**:

Отступление от темы: у меня уже начинаются убиваться процесссы на виртуалке из-за нехватки опертивки.

Теперь выставляем внешний шлюз для роутера **demo-router**:

Вот примерные параметры роутера **demo-router**:

10.2. Теперь переходим к созданию экземпляра ВМ:

А, попались, думали уже можно создать ВМ, неа еще надо создать **flavour** (шаблон виртуальной машины по выделяемым ресурсам) перед этим. Шаблогы по умолчанию может создавать только админ:

- **--ram** — оперативная память
- **--disk** — размер диска
- **--vcpu** — количество витуальных ядер CPU
- **--publc** — доступен всем пользователям
- **--rxtx-factor** — пропускная способность сети (только для гипервизора XEN)
- **--ephemeral** — размер второго диска (в отличии от --disk всегда удаляется при удалении ВМ)
- **--swap** — размер опционального [**swap-раздела**](https://fornex.com/ru/help/swap/#:~:text=SWAP%20\(своп\)%20—%20это%20механизм,%2C%20называемые%20страницами%20\(pages\).)

Выделил контроллер узлу еще 2GB оперативки, надеюсь хватит, но я уже вхожу в зону «выделил ресурсов больше чем на самом деле» (2+2+6=8)

Теперь создадим пару ssh ключей, которую служба метаданных сервиса Nova будет передавать созданным ВМ:

Вхожу в теневую зону (запускаю сразу 3 ВМ) и пытаюсь создать ВМ по созданному шаблону **m2.tiny**, передав в нее созданную пару ключей **demokey1** (если на этом отчет закончился, то это потому что у меня взорвался ноубук):

При запуске обоих рабочих узлов, узел управления сразу падает, видимо kvm не любет избыточность ресурсов, поэтому прощай **compute-opt**. Лагануло при создании ВМ у меня знатно.

**Изливаю очередной бомбеж:** я весь день (5 часов подряд) пытался пофикисть ошибку **UnicodeDecodeError**, которая вылезала при создании ВМ, к 5 часу я уже редачил питоновские файлы по гайдам на китайском языке, но все равно ошибка коварно вылезала, а потом я перезагрузил (в 3 раз за 5 часоа) виртуалки и все пофиксилось, вот как так то...

Посмотреть статус созданной ВМ можно как с помощью **openstack cli**, так и при помощи **virsh** на рабочем узле:


